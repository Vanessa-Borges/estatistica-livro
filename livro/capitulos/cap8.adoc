== Introdução à Inferência Estatística

=== Definições Básicas

Vamos começar relembrando dois conceitos básicos importantes de estatística, a saber, _população_ e _amostra_.

(((População)))
(((Amostra)))

De uma maneira mais precisa, temos a seguinte definição:

Definição: População::
+
--
O conjunto de todos os elementos, ou resultados, sob investigação é chamado de população.
--

Quando estamos lidando com uma população é interessante observar:

* Características mensuráveis (expressas por variáveis numéricas);

* Características qualitativas (expressas por variáveis nominais ou categóricas).

(((Parâmetros Populacionais)))

Definição: Parâmetros Populacionais::
+
--
Damos o nome de parâmetros da população ou parâmetros populacionais aos valores numéricos que caracterizam globalmente
uma população.
--

(((Amostra, Tamanho)))

Relacionadas à população temos as seguintes definições:

Definição: Amostra e Tamanho Amostral::
+
--
Um subconjunto da população é chamado de _amostra_. Chamamos o número de elementos da amostra de de
_tamanho amostral_.
--

[IMPORTANT]
====
O objetivo da inferência estatística é produzir afirmações sobre dada característica da população na qual estamos
interessados, a partir de informações colhidas de uma parte dessa população. Esta característica na 
população pode ser representada por uma _variável aleatória_.
====

A relação entre inferência e amostragem é ilustrada na figura abaixo:

.Amostragem e Inferência
image::images/inferencia/amos-infe.eps[scaledwidth="60%"] 

[NOTE]
====
* Se tivermos informações completas sobre a distribuição, não haverá necessidade de obter amostras.

* Podemos supor que as variáveis vêm de uma família de distribuições de probabilidade, mas não podemos
supor qual o valor do parâmetro. Por exemplo, podemos supor que os dados seguem distribuição normal,
mas não podemos informar os valores das médias e variâncias.

* Existem casos onde a amostragem é necessária. Por exemplo, se quisermos saber o número de glóbulos brancos.

* É importante que a amostra seja representativa da população, ou seja, que o comportamento da amostra seja próximo
do comportamento da população. Para garantir isso, é preciso saber escolher bem o tamanho amostral, e que a amostra
seja obtida aleatoriamente. 
====

=== Amostragem

(((Amostragem)))

==== Tipos de Amostragem

(((Amostragem, Probabilística)))
(((Amostragem, Não-Probabilística)))

Temos dois grandes grupos de amostragem:

* _Amostragem Probabilística_: O mecanismo de escolha dos elementos da amostra é tal 
que existe uma probabilidade conhecida de cada elemento da população
vir a participar da amostra.

* _Amostragem Não-Probabilística_: Não existe nenhum mecanismo probabilístico na seleção da amostra.

.Tipos de Amostragem Probabilística

(((Amostragem, Aleatória Simples)))

* *Amostragem Aleatória Simples* (AAS):
.. Supomos que a população é homogênea, ou seja, a característica que estamos procurando pode aparecer
em qualquer elemento da população com a mesma probabilidade;
.. Procedimento: Rotular os elementos da população e sortear os indivíduos que farão parte da amostra.

(((Amostragem, Sistemática)))

* *Amostragem Sistemática*:
.. Supomos que a população é homogênea;
.. Procedimento: Os elementos da população são ordenados, a retirada do primeiro elemento é aleatória, e 
a partir do segundo elemento a retirada é feita periodicamente (com período determinístico). Por exemplo,
o primeiro elemento é retirado aleatoriamente, e em seguida, retiramos o décimo elemento depois do primeiro
retirado, depois o décimo elemento após o segundo retirado, e assim por diante.

(((Amostragem, Estratificada)))

* *Amostragem Estratificada*:
.. Supomos que a população é heterogênea, ou seja, a característica que estamos procurando pode variar dependendo de 
onde os dados são retirados. Entretanto, supomos que podemos dividir a população em grupos (estratos) homogêneos;
.. Procedimento: A seleção dos elementos de cada estrato é realizada de forma aleatória, ou seja, realizamos uma
amostragem aleatória simples em cada estrato.

(((Amostragem, por Conglomerado)))

* *Amostragem por Conglomerado*:
.. Supomos que a população pode ser dividida em subgrupos (conglomerados) heterogêneos;
.. Procedimento: A amostragem é realizada sobre os conglomerados, e não mais sobre os indivíduos da população,
ou seja, realiza-se uma amostragem aleatória simples, onde os elementos escolhidos são os conglomerados a serem utilizados,
ao invés de já se sortear os elementos da amostra.

==== Distribuição Amostral

*Interesse*::
+
--
Uma medida que descreva certa característica da população. Normalmente temos interesse em um 
parâmetro desconhecido da população, seja média, variância, ou outro parâmetro.
--

(((Estatística)))

_Solução_::
+
--
A partir da amostra, podemos construir uma função, utilizando apenas os valores obtidos nesta amostra, para
descrever tal característica. Esta função é chamada de *estatística*.
--

[NOTE]
====
Como os valores da amostra são aleatórios, qualquer quantidade calculada em função dos elementos da amostra também 
será uma variável aleatória. Assim, as estatísticas, sendo variáveis aleatórias, terão alguma distribuição de probabilidade.
====

.Formalização do Problema

(((AAS)))

Seja x:[X_1,\ldots,X_n] uma amostra aleatória simples (AAS) de uma população de tamanho x:[n]. Para realizarmos uma afirmação
sobre algum parâmetro x:[\theta] da população (média, variância, etc.), utilizaremos uma estatística x:[T] que, como sabemos,
é uma função da amostra, isto é, x:[T=f(X_1,\ldots,X_n)], para alguma função x:[f].

(((Distribuição Amostral)))

[NOTE]
====
Quando conhecemos melhor o comportamento da estatística x:[T], ou seja, se conhecemos sua _distribuição amostral_, que
nada mais é que a distribuição de probabilidade da variável aleatória x:[T], poderemos realizar afirmações sobre o
parâmetro x:[\theta]. 


A distribuição amostral relata o comportamento da estatística x:[T], caso retirássemos todas as possíveis amostras de tamanho
x:[n].
====

===== Distribuição Amostral da Média

(((Média Amostral)))

Consideremos uma população identificada pela variável aleatória x:[X], cujos parâmetros _média populacional_ 
x:[\mu = E(X)] e variância populacional x:[\sigma^2 = Var(X)] são supostos conhecidos. Vamos tirar
todas as possíveis amostras de tamanho x:[n] dessa população e, para ccada uma, calcular a média
amostral x:[\overline{X}] dada por
[latexmath]
++++
\[
\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i,
\]
++++
e em seguida vamos obter algumas propriedades de x:[\overline{X}].

.Exemplo de distribuição amostral de x:[\overline{X}]
====
Considere a população x:[\{1,3,5,7\}]. Sabemos que a média populacional é x:[\mu = 4,2] e a variância populacional é
x:[\sigma^2 = 4,16]. Se retiramos uma amostra de tamanho x:[n=2], segundo amostragem aleatória simples (então
todos os elementos possuem a mesma probabilidade de serem retirados), a distribuição amostral de 
[latexmath]
++++
\[
\overline{X} = \frac{X_1+X_2}{2}
\]
++++
será dada por

[format="csv",cols="3,2,2,2,2,2,2,2,2",frame="topbot",grid="none"]
|=======================
x:[\overline{x}],  1,2,3,4,5,6,7  ,Total
x:[P(\overline{X}=\overline{x})],    x:[1/25], x:[2/25],x:[5/25],x:[6/25],x:[6/25],x:[4/25],x:[1/25]     ,1
|=======================

Assim, 
[latexmath]
++++
\[
E(\overline{X}) = \sum_{i=1}^7 \overline{x}_iP(\overline{X}=\overline{x}_i) = 4,2,
\]
++++
e
[latexmath]
++++
\[
Var(\overline{X}) = 2,08.
\]
++++

====

Temos então a seguinte proposição:

Proposição::
+
--
Seja x:[X] uma variável aleatória com média x:[\mu] e variância x:[\sigma^2], e seja x:[(X_1,\ldots,X_n)]
uma AAS de x:[X]. Então,
[latexmath]
++++
\[
E(\overline{X}) = \mu\quad\hbox{~e~} Var(\overline{X}) = \frac{\sigma^2}{n}.
\]
++++
--

_Demonstração_::
+
--
Temos que
[latexmath]
++++
\[
\begin{array}{lll}
E(\overline{X}) &=& \displaystyle E\Big(\frac{1}{n}\sum_{i=1}^n X_i\Big)\\
&=& \displaystyle\sum_{i=1}^n \frac{E(X_i)}{n}\\
&=& \displaystyle\frac{1}{n} \sum_{i=1}^n \mu\\
&=& \displaystyle\frac{n\mu}{n}\\
&=&\mu.
\end{array}
\]
++++
e, usando que a variância de soma de variáveis independentes é dada pela
soma das variâncias, e as propriedades da variância, temos:
[latexmath]
++++
\[
\begin{array}{lll}
Var(\overline{X}) &=& \displaystyle Var\Big(\frac{1}{n}\sum_{i=1}^n X_i\Big)\\
&=& \displaystyle\sum_{i=1}^n Var\Big(\frac{X_i}{n}\Big)\\
&=& \displaystyle\frac{1}{n^2} \sum_{i=1}^n \sigma^2\\
&=& \displaystyle\frac{n\sigma^2}{n^2}\\
&=&\displaystyle\frac{\sigma^2}{n}.
\end{array}
\]
++++

--

===== Teorema Central do Limite

(((Teorema, Central do Limite)))

Vamos agora enunciar um dos principais resultados da probabilidade moderna: o teorema central do limite. 
A demonstração deste teorema pode ser encontrada em livros mais avançados de probabilidade.

Teorema Central do Limite::
+
--
Sejam x:[X_1,\ldots,X_n] uma AAS da variável aleatória x:[X], com distribuição comum satisfazendo
x:[E(X_i) = \mu] e x:[Var(X_i)=\sigma^2]. Como a amostragem foi AAS, temos que as variáveis são independentes.
Assim, se x:[n] é grande, temos que,
[latexmath]
++++
\[
P(\overline{X}\leq x) \approx \Phi_{\mu,\sigma^2/n}(x),
\]
++++
onde x:[\Phi_{\mu,\sigma^2}] é a função de distribuição de uma variável aleatória x:[N\sim N(\mu,\sigma^2/n)]. 

Assim, dizemos que x:[\overline{X}] segue aproximadamente distribuição normal com média x:[\mu] e variância
x:[\sigma^2/n]. 
--

Podemos fazer a mudança de variáveis:
[latexmath]
++++
\[
Z = \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}.
\]
++++

Desta forma, o teorema central do limite nos diz que se x:[n] é suficientemente grande, temos que x:[Z] segue 
aproximadamente distribuição normal com média 0 e variância 1.

[NOTE]
====
No caso em que a distribuição de x:[X] é normal, a distribuição de x:[\overline{X}] será normal, mesmo para valores pequenos de x:[n].
====

===== Distribuição Amostral da Proporção

Seja x:[X] uma variável aleatória com distribuição Bernoulli com parâmetro x:[p], isto é, 
x:[P(X=1)=p] e x:[P(X=0)=1-p]. Temos que x:[E(X)=p] e x:[Var(X) = p(1-p)]. Considere
uma AAS de tamanho x:[n] dessa população. Seja 
[latexmath]
++++
\[
S_n = \sum_{i=1}^n X_i,
\]
++++
o número de indivíduos com a característica de interesse da amostra. Sabemos que 
x:[S_n\sim Bin(n,p)]. 

Pelo teorema central do limite temos que x:[\overline{X}] tem distribuição aproximadamente normal,
para x:[n] suficientemente grande. Seja x:[\widehat{p} = \overline{X}], a proporção amostral. Então, 
temos que
[latexmath]
++++
\[
\widehat{p} \stackrel{aprox.}{\sim} N\Big( p, \frac{p(1-p)}{n}\Big),
\]
++++
ou equivalentemente,
[latexmath]
++++
\[
Z = \frac{\widehat{p}-p}{\sqrt{p(1-p)/n}} \stackrel{aprox.}{\sim} N(0,1),
\]
++++
pois, temos que
[latexmath]
++++
\[
E(\widehat{p}) = E\Big(\frac{S_n}{n}\Big) = \frac{1}{n} E(S_n) = n\frac{p}{n} = p,
\]
++++
e
[latexmath]
++++
\[
Var(\widehat{p}) = Var\Big(\frac{S_n}{n}\Big) = \frac{1}{n^2} Var(S_n) = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}.
\]
++++

===== Distribuição Amostral da Diferença entre Médias

Em vários problemas práticos, deseja-se comparar duas populações de interesse. Por exemplo,
podemos estar interessados em avaliar a diferença de desempenho entre duas linhas de produção.

Suponha que duas populações de interesse, x:[X_1] e x:[X_2], com médias x:[\mu_1] e x:[\mu_2],
e variâncias x:[\sigma_1^2] e x:[\sigma_2^2], respectivamente. 

Considere duas AAS independentes de tamanhos x:[n_1] e x:[n_2] das duas populações. Pelo
teorema central do limite, a distribuição amostral da diferença x:[(\overline{X}_1-\overline{X}_2)],
para x:[n_1] e x:[n_2] suficientemente grandes, será dada por
[latexmath]
++++
\[
(\overline{X}_1-\overline{X}_2)\stackrel{aprox.}{\sim} N\Big(\mu_1-\mu_2,\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}\Big),
\]
++++
ou equivalentemente,
[latexmath]
++++
\[
Z = \frac{(\overline{X}_1-\overline{X}_2)-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}}  \stackrel{aprox.}{\sim} N(0,1),
\]
++++
pois,
[latexmath]
++++
\[
E(\overline{X}_1-\overline{X}_2) = E(\overline{X}_1) - E(\overline{X}_2) = \mu_1-\mu_2,
\]
++++
e
[latexmath]
++++
\[
Var(\overline{X}_1-\overline{X}_2) = Var(\overline{X}_1)+Var(\overline{X}_2)=\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}.
\]
++++

===== Distribuição Amostral da Diferença entre Proporções

Neste caso, supomos que as duas populações de interesse apresentam distribuição binomial com proporções x:[p_1] e x:[p_2].
Considere que são feitas duas AAS independentes de tamanhos x:[n_1] e x:[n_2]. A distribuição amostral da diferença
entre proporções x:[(\widehat{p}_1-\widehat{p}_2)], para x:[n_1] e x:[n_2] suficientemente grandes, pelo teorema
central do limite temos
[latexmath]
++++
\[
(\widehat{p}_1-\widehat{p}_2)\stackrel{aprox.}{\sim} N\Big(p_1-p_2,\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}\Big),
\]
++++
ou equivalentemente,
[latexmath]
++++
\[
Z = \frac{(\widehat{p}_1-\widehat{p}_2)-(p_1-p_2)}{\sqrt{p_1(1-p_1)/n_1+p_2(1-p_2)/n_2}}  \stackrel{aprox.}{\sim} N(0,1).
\]
++++


=== Inferência Estatística

Suponha que alguma característica da população possa ser representada por uma  variável aleatória x:[X],
com função de distribuição x:[F_X(x;\theta)]. Suponha que os valores x:[x_1,\ldots,x_n] de uma AAS
x:[X_1,\ldots,X_n] de x:[F_X(x;\theta)] possam ser observados. Com base nos valores amostrais,
desejamos estimar o parâmetro desconhecido x:[\theta], ou alguma função deste parâmetro.
Neste caso, a estimação poderá ser feita de duas maneiras:

(((Estimação, Pontual)))
(((Estimação, Intervalar)))
(((Estimador)))

* *Estimação Pontual*: Estimamos o parâmetro x:[\theta] por meio de uma estatística x:[T=t(X_1,\ldots,X_n)],
chamada de _estimador_.

* *Estimação Intervalar*: É definida por duas estatísticas x:[T_1 = t_1(X_1,\ldots,X_n)] e
x:[T_2=t_2(X_1,\ldots,X_n)], tais que x:[T_1<T_2], onde o intervalo x:[ [T_1,T_2\]] terá uma probabilidade
conhecida de conter o parâmetro desconhecido x:[\theta]. 

==== Estimação Pontual

(((Estimativa)))

Vamos começar entendendo a diferença entre _estimador_ e _estimativa_.

Definição: Estimador::
+
--
Um estimador é uma estatística, isto é, é uma função da amostra, que é usada para representar
um valor plausível para o parâmetro desconhecido de interesse.
--

Definição: Estimativa::
+
--
É valor numérico particular assumido por um estimador. Ou seja, é o valor do estimador aplicado em uma realização da amostra.
--

==== Propriedades dos Estimadores

IMPORTANT: É importante frisar que podem existir vários estimadores para um mesmo parâmetro populacional.
Logo, a escolha do melhor estimador será feita com base em alguns critérios.

(((Estimador, Não-tendencioso)))
(((Estimador, Não-viesado)))
(((Estimador, Não-viciado)))

* *Não-Tendencioso* (Também chamados de não-viesados ou não-viciados): Dizemos que um estimador
x:[T] é não-viesado para o parâmetro x:[\theta] se o seu valor esperado for igual ao próprio parâmetro,
isto é, se 
[latexmath]
++++
\[
E(T) = \theta.
\]
++++

(((Consistência)))
(((Estimador, Consistente)))

* *Consistência*: Dizemos que um estimador x:[T] para o parâmetro x:[\theta] é _consistente_ se, além de ser não-viesado,
sua variância tende a zero quando o tamanho amostral tende a infinito:
[latexmath]
++++
\[
\lim_{n\to\infty} Var(T) = 0.
\]
++++

(((Estimador, Eficiente)))
(((Eficiência)))

* *Eficiência*: Sejam x:[T_1] e x:[T_2] dois estimadores não-viesados para o parâmetro x:[\theta], com
[latexmath]
++++
\[
Var(T_1) < Var(T_2),
\]
++++
então, dizemos que x:[T_1] é mais eficiente que x:[T_2].

.Exemplo de estimador viesado
====
Seja x:[X_1,\ldots,X_n] uma AAS da seguindo distribuição uniforme no intervalo x:[[0,\theta\]]. 
Um estimador natural para x:[\theta] é dado pelo maior valor encontrado na amostra, já que sabemos que a distribuição
uniforme não fornece valores maiores do que x:[\theta].

Assim, seja x:[M = max(X_1,\ldots,X_n)], ou seja, o maior valor da amostra. Vamos mostrar que x:[M] é um estimador
viesado para x:[\theta].

Seja x:[X\sim U(0,\theta)], então a função de densidade de x:[X] é dada por
[latexmath]
++++
\[
f_X(x) = \frac{1}{\theta},\quad 0<x<\theta,
\]
++++
e x:[f_X(x) = 0] caso contrário. Assim, se x:[F_M] é a função de distribuição de x:[M], então,
como as variáveis x:[X_1,\ldots,X_n] são independentes, temos que
[latexmath]
++++
\[
\begin{array}{lll}
F_M(m) &=& P(M\leq m) = P(max(X_1,\ldots,X_n)\leq m)\\
&=& P(X_1\leq m,\ldots, X_n\leq m) = P(X_1\leq m)\cdots P(X_n\leq m)\\
&=& [P(X\leq m)]^n = [F_X(m)]^n,
\end{array}
\]
++++
e portanto,
[latexmath]
++++
\[
f_M(m) = F_M'(m) = n [F_X(m)]^{n-1} f_X(m).
\]
++++
Além disso, temos que
[latexmath]
++++
\[
F_X(x) = \int_0^x \frac{1}{\theta} dt = \frac{x}{\theta}, \quad 0<x<\theta.
\]
++++
Logo, temos que
[latexmath]
++++
\[
f_M(m) = n\Big[\frac{m}{\theta}\Big]^{n-1} \frac{1}{\theta} = \frac{nm^{n-1}}{\theta^n},\quad 0<m<\theta.
\]
++++

[latexmath]
++++
\[
\begin{array}{lll}
E(M) &=& \displaystyle \int_0^\theta m \frac{nm^{n-1}}{\theta^n} dm = \frac{n}{\theta^n}\int_0^\theta m^ndm\\
&=& \displaystyle\frac{n}{\theta^n} \Big(\frac{m^{n+1}}{n+1}\Big)\Big|_0^\theta\\
&=& \displaystyle\frac{n}{\theta^n}\frac{\theta^{n+1}}{n+1}\\
&=& \displaystyle\frac{n}{n+1} \theta.
\end{array}
\]
++++
Assim, temos que x:[M] é um estimador viesado. Podemos obter um outro estimador, a partir de x:[M], que seja não-viesado,
dado por
[latexmath]
++++
\[
\widetilde{M} = \frac{n+1}{n} M.
\]
++++
====


==== Alguns Estimadores Pontuais Importantes

===== Estimador para a Média

O estimador mais utilizado para a média populacional x:[\mu] é a média amostral:
[latexmath]
++++
\[
\widehat{\mu} = \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i.
\]
++++

===== Estimador para a Variância

Quando a média populacional x:[\mu] é conhecida, um estimador para a variância populacional é dado
por
[latexmath]
++++
\[
\widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i-\mu)^2.
\]
++++

Caso a média populacional x:[\mu] seja desconhecida, que é a situação mais comum na prática, a
variância populacional pode ser estimada por
[latexmath]
++++
\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2.
\]
++++

===== Estimador para a Proporção

Um estimador para a proporção populacional é dado pela proporção amostral:
[latexmath]
++++
\[
\widehat{p} = \frac{S_n}{n},
\]
++++
onde x:[S_n] é o número de elementos que apresentam uma determinada característica de interesse entre os x:[n] elementos
da amostra.



=== Regressão e Correlação

(((Correlação)))

==== Correlação

(((Relação, Funcional)))
(((Relação, Estatística)))

.Relação Funcional e Relação Estatística

Como sabemos, o perímetro e o lado de um quadrado estão relacionados. A relação que os liga é perfeitamente definida
e pode ser expressa matematicamente por
[latexmath]
++++
\[
\hbox{perímetro} = 4l,
\]
++++
onde x:[l] é o lado do quadrado. Atribuindo-se, então, um valor qualquer a x:[l], é possível determinar _exatamente_
o valor do perímetro.

Consideremos agora a relação entre o peso e a altura de um grupo de pessoas. É evidente que esta relação não é do mesmo tipo da anterior.
Assim, podemos ter duas pessoas com a mesma altura e pesos diferentes, assim como pessoas com mesmo peso e alturas diferentes. Porém,
existe uma tendência clara de que, quanto maior a altura, maior o peso.

As relações do tipo perímetro-lado são chamadas de _relações funcionais_ e as relações do tipo peso-altura são chamadas de
_relações estatística_.

Quando duas variáveis estão ligadas por uma relação estatística, dizemos que existe *correlação* entre elas.

===== Diagrama de Dispersão

O diagrama de dispersão apresenta os pares ordenados x:[(x_i,y_i)] de uma amostra aleatória bidimensional em um plano cartesiano.
Esse diagrama nos fornece uma ideia grosseira, porém útil, da correlação existente.


.Exemplo de diagrama de dispersão
image::images/inferencia/disper1.eps[scaledwidth="60%"] 

.Correlação Linear

(((Correlação Linear, Positiva)))
(((Correlação Linear, Negativa)))
(((Correlação Não-Linear)))

* Se os pontos do diagrama apresentam uma tendência linear ascendente, temos correlação linear positiva:

.Exemplo de diagrama de dispersão com correlação linear positiva
image::images/inferencia/disper2.eps[scaledwidth="60%"] 

* Se os pontos apresentam uma tendência linear descendente, temos correlação linear negativa
.Exemplo de diagrama de dispersão com correlação linear negativa
image::images/inferencia/disper3.eps[scaledwidth="60%"] 

* Se os pontos apresentam uma tendência curvilínea, temos correlação não-linear
.Exemplo de diagrama de dispersão com correlação não-linear
image::images/inferencia/disper4.eps[scaledwidth="60%"] 

* Se os pontos apresentam-se dispersos, não oferecendo uma tendência definida, concluímos que não há correlação entre as variáveis
em estudo

.Exemplo de diagrama de dispersão sem correlação
image::images/inferencia/disper5.eps[scaledwidth="60%"] 

===== Coeficiente de Correlação de Pearson

(((Correlação, de Pearson)))

É usado para indicar o grau de intensidade da correlação _linear_ entre duas variáveis e, ainda, o sentido dessa correlação: 
se positivo ou negativo. O coeficiente de correlação de Pearson entre duas amostras x:[(x_1,\ldots,x_n)]
e x:[(y_1,\ldots,y_n)] é dado por 

[latexmath]
++++
\[
r = \frac{n\sum_{i=1}^n x_iy_i - \big( \sum_{i=1}^n x_i \big) \big(\sum_{i=1}^n y_i\big)}{\sqrt{\Big[n\sum_{i=1}^nx_i^2-\big(\sum_{i=1}^n x_i\big)^2\Big]\Big[n\sum_{i=1}^n y_i^2 - \big(\sum_{i=1}^n y_i\big)^2\Big]}},
\]
++++
onde x:[n] é o número de observações. Observe que x:[r\in [-1,1\]]. 

Temos que

* Se x:[r=1], há uma correlação perfeita e positiva entre as variáveis.

* Se x:[r=-1] há uma correlação perfeita e negativa entre as variáveis.

* Se x:[r=0] não há correlação entre as variáveis.

.Exemplo de cálculo do coeficiente de correlação de Pearson
====
Considere uma amostra aleatória das variáveis x:[(X,Y)], dada por x:[(x_i,y_i)] na tabela abaixo:

[options="header,footer"]
|==========================
|      x:[x_i]|x:[y_i] | x:[x_iy_i] | x:[x_i^2] | x:[y_i^2]
| 4|12|48|16|144
|6|10|60|36|100
|8|8|64|64|64
|10|12|120|100|144
|12|14|168|144|196
|x:[\sum x_i]| x:[\sum y_i] | x:[\sum x_iy_i] | x:[\sum x_i^2] | x:[\sum y_i^2]
|40|56|460|360|648
|==========================

Assim, temos x:[n=5], e portanto

[latexmath]
++++
\[
r = \frac{5\times 460 - 40\times 56}{\sqrt{(5\times 360-(40)^2)(5\times 648-(56)^2)}} = 0,4160.
\]
++++

Logo, a correlação linear entre as variáveis x:[X] e x:[Y] é positiva, porém fraca.
====

==== Regressão

Podemos dizer que a análise de regressão tem como objetivo descrever, através de um modelo matemático, a relação entre duas
variáveis.

(((Variável, Dependente)))
(((Variável, Independente)))

A variável sobre a qual desejamos fazer uma estimativa recebe o nome de _variável dependente_ e a outra variável
recebe o nome de _variável independente_.

Assim, supondo que x:[X] é a variável independente e x:[Y] é a variável dependente, procuramos determinar através de ajuste de uma reta
a relação entre essas variáveis, ou seja, vamos obter a função definida por
[latexmath]
++++
\[
Y = a + bX,
\]
++++
onde x:[a] e x:[b] são os parâmetros da regressão. Entretanto, sabemos que essa fórmula não é exata,
assim, existe a presença de um erro aleatório:
[latexmath]
++++
\[
Y_i = a + bX_i + e_i,\quad i=1,\ldots,n,
\]
++++
onde x:[e_i] é um erro aleatório que possui valor esperado igual a zero. 

A maneira que vamos utilizar para determinar valores adequados para x:[a] e x:[b] é a de minimizar
a soma de quadrado dos erros. Ou seja, queremos escolher os valores x:[a] e x:[b] de tal forma
que o nosso modelo ``erre'' pouco.

(((Método, Mínimos Quadrados)))

Este método é chamado de _método de mínimos quadrados_. Assim, dadas as observações x:[(X_i,Y_i),i=1,\ldots,n], desejamos minimizar
[latexmath]
++++
\[
\sum_{i=1}^n e_i^2 = \sum_{i=1}^n (Y_i-a-bX_i)^2.
\]
++++

Desta forma, para encontrarmos o ponto de mínimo, precisamos calcular as derivadas parciais:
[latexmath]
++++
\[
\frac{\partial \sum_{i=1}^n e_i^2}{\partial a} = -2 \sum_{i=1}^n (Y_i-a-bX_i),
\]
++++
e
[latexmath]
++++
\[
\frac{\partial \sum_{i=1}^n e_i^2}{\partial b} = -2 \sum_{i=1}^n (Y_i-a-bX_i)X_i.
\]
++++
Assim, como os nossos estimadores x:[\widehat{a}] e x:[\widehat{b}] são os valores que minimizam a soma de quadrados 
dos erros, temos que x:[\widehat{a}] e x:[\widehat{b}] são tais que as derivadas parciais calculadas acima se anulam.

Logo, temos que:
[latexmath]
++++
\[
-2\sum_{i=1}^n (Y_i-\widehat{a}-\widehat{b}X_i) = 0 \Rightarrow \sum_{i=1}^n Y_i - n\widehat{a} - \widehat{b}\sum_{i=1}^n X_i = 0 \Rightarrow \frac{1}{n}\sum_{i=1}^n Y_i = \widehat{a} + \widehat{b} \frac{1}{n}\sum_{i=1}^n X_i,
\]
++++
e portanto
[latexmath]
++++
\[
\widehat{a} = \overline{Y} - \widehat{b}\overline{X}.
\]
++++

Por outro lado, temos também que
[latexmath]
++++
\[
-2\sum_{i=1}^n (Y_i-\widehat{a}-\widehat{b}X_i)X_i = 0 \Rightarrow \sum_{i=1}^n Y_iX_i - \widehat{a}\sum_{i=1}^n X_i - \widehat{b}\sum_{i=1}^nX_i^2 =0,
\]
++++
daí,
[latexmath]
++++
\[
\sum_{i=1}^n Y_iX_i = \widehat{a} \sum_{i=1}^n X_i + \widehat{b} \sum_{i=1}^n X_i^2.
\]
++++

Substituindo o valor de x:[\widehat{a}] na equação acima, obtemos
[latexmath]
++++
\[
\sum_{i=1}^n Y_iX_i = (\overline{Y}-\widehat{b}\overline{X})\sum_{i=1}^n X_i + \widehat{b}\sum_{i=1}^n X_i^2.
\]
++++
Isolando x:[\widehat{b}], obtemos
[latexmath]
++++
\[
\widehat{b}\Big(\sum_{i=1}^n X_i^2 - \frac{\big(\sum_{i=1}^n X_i\big)^2}{n}\Big) = \sum_{i=1}^n Y_iX_i - \frac{\big(\sum_{i=1}^n Y_i\big)\big(\sum_{i=1}^n X_i\big)}{n}.
\]
++++
Isto nos fornece
[latexmath]
++++
\[
\widehat{b} = \frac{\sum_{i=1}^n Y_iX_i - \big(\sum_{i=1}^n Y_i\big)\big(\sum_{i=1}^n X_i\big)/n}{\sum_{i=1}^n X_i^2 - \big(\sum_{i=1}^n X_i\big)^2/n}
\]
++++

Costuma-se usar as seguintes notações para o numerador e denominador da expressão que define x:[\widehat{b}]:
[latexmath]
++++
\[
S_{YX} = \sum_{i=1}^n Y_iX_i - \frac{\big(\sum_{i=1}^n Y_i\big)\big(\sum_{i=1}^n X_i\big)}{n},
\]
++++
e
[latexmath]
++++
\[
S_{XX} = \sum_{i=1}^n X_i^2 - \frac{\big(\sum_{i=1}^n X_i\big)^2}{n}.
\]
++++

Assim, temos as fórmulas para x:[\widehat{b}] e x:[\widehat{a}] em notação simplificada:

[latexmath]
++++
\[
\widehat{b} = \frac{S_{YX}}{S_{XX}}\quad\hbox{~e~}\quad \widehat{a} = \overline{Y} - \widehat{b}\overline{X}.
\]
++++

[IMPORTANT]
====
Como estamos fazendo uso de uma amostra para obtermos os valores dos parâmetros, o resultado, na realidade, é um
_estimador_ para a verdadeira equação de regressão, e portanto, temos
[latexmath]
++++
\[
\widehat{Y}_i = \widehat{a} + \widehat{b}X_i,
\]
++++
onde x:[\widehat{Y}_i] é um estimador para x:[Y_i].
====

.Exemplo de cálculo das estimativas dos parâmetros em um modelo de regressão
====
Abaixo apresentamos os valores de uma amostra de 10 observações de duas variáveis aleatórias x:[X] e x:[Y]:

[options="header,footer"]
|==========================
|      x:[y_i]|x:[x_i] | x:[y_ix_i] | x:[x_i^2] 
| 6|5|30|25
|9|8|72|64
|8|7|56|49
|10|10|100|100
|5|6|30|36
|7|7|49|49
|8|9|72|81
|4|3|12|9
|6|8|48|64
|2|2|4|4
|x:[\sum x_i]| x:[\sum y_i] | x:[\sum x_iy_i] | x:[\sum x_i^2] 
|65|65|473|481
|==========================

Daí,
[latexmath]
++++
\[
S_{YX} = 473 - \frac{65^2}{10} = 473-422,5 = 50,5,
\]
++++
e
[latexmath]
++++
\[
S_{XX} = 481-\frac{65^2}{10} = 481 - 422,5 = 58,5,
\]
++++
assim
[latexmath]
++++
\[
\widehat{b} = \frac{50,5}{58,5} = 0,86\quad\hbox{~e~} \quad \widehat{a} = \frac{65}{10}-0,86\cdot\frac{65}{10} = 0,91.
\]
++++
Logo, temos a equação
[latexmath]
++++
\[
\widehat{Y}_i = 0,91 + 0,86X_i.
\]
++++

A figura abaixo mostra o diagrama de dispersão juntamente com a reta de regressão estimada acima:



====






















